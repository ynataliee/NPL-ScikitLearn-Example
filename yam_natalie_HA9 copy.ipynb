{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02afc13-9124-4791-be5d-748902cc1147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text sentiment\n",
      "0            Boring plot and unconvincing characters.  Negative\n",
      "1           The acting was superb! A must-watch film.  Positive\n",
      "2   I have mixed feelings about this film. It had ...   Neutral\n",
      "3       Not great, not terrible. It's a decent watch.   Neutral\n",
      "4             An outstanding performance by the cast.  Positive\n",
      "5   I loved every moment of this movie. Highly rec...  Positive\n",
      "6   The movie was okay. It didn't leave a strong i...   Neutral\n",
      "7     Terrible acting. I couldn't wait for it to end.  Negative\n",
      "8                 The worst movie I've seen in years.  Negative\n",
      "9     A heartwarming story that left me with a smile.  Positive\n",
      "10                A complete waste of time and money.  Negative\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Sample positive and negative movie reviews\n",
    "positive_reviews = [\"The acting was superb! A must-watch film.\",\n",
    "                    \"I loved every moment of this movie. Highly recommended.\",\n",
    "                   \"An outstanding performance by the cast.\",\n",
    "                    \"A heartwarming story that left me with a smile.\"]\n",
    "\n",
    "negative_reviews = [\"Terrible acting. I couldn't wait for it to end.\",\n",
    "                    \"Boring plot and unconvincing characters.\",\n",
    "                   \"A complete waste of time and money.\",\n",
    "                    \"The worst movie I've seen in years.\"]\n",
    "\n",
    "neutral_reviews = [\n",
    "    \"The movie was okay. It didn't leave a strong impression.\",\n",
    "    \"I have mixed feelings about this film. It had its moments.\",\n",
    "    \"Not great, not terrible. It's a decent watch.\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {'text': positive_reviews + negative_reviews + neutral_reviews, 'sentiment': ['Positive'] * len(positive_reviews) + ['Negative'] * len(negative_reviews) + ['Neutral'] * len(neutral_reviews)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Shuffle the DataFrame to mix positive and negative reviews\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0a48859-0507-4be9-a31e-99bd45aeb372",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/natalieyam/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>processed</th>\n",
       "      <th>length</th>\n",
       "      <th>words</th>\n",
       "      <th>words_not_stopword</th>\n",
       "      <th>tagged_words</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>commas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boring plot and unconvincing characters.</td>\n",
       "      <td>Negative</td>\n",
       "      <td>boring plot and unconvincing characters</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>[(boring, NN), (plot, NN), (and, CC), (unconvi...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The acting was superb! A must-watch film.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>the acting was superb a mustwatch film</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>[(the, DT), (acting, NN), (was, VBD), (superb,...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have mixed feelings about this film. It had ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>i have mixed feelings about this film it had i...</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>[(i, NNS), (have, VBP), (mixed, VBN), (feeling...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not great, not terrible. It's a decent watch.</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>not great not terrible its a decent watch</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>[(not, RB), (great, JJ), (not, RB), (terrible,...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>An outstanding performance by the cast.</td>\n",
       "      <td>Positive</td>\n",
       "      <td>an outstanding performance by the cast</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>[(an, DT), (outstanding, JJ), (performance, NN...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "0           Boring plot and unconvincing characters.  Negative   \n",
       "1          The acting was superb! A must-watch film.  Positive   \n",
       "2  I have mixed feelings about this film. It had ...   Neutral   \n",
       "3      Not great, not terrible. It's a decent watch.   Neutral   \n",
       "4            An outstanding performance by the cast.  Positive   \n",
       "\n",
       "                                           processed  length  words  \\\n",
       "0            boring plot and unconvincing characters      39      5   \n",
       "1             the acting was superb a mustwatch film      38      7   \n",
       "2  i have mixed feelings about this film it had i...      56     11   \n",
       "3          not great not terrible its a decent watch      41      8   \n",
       "4             an outstanding performance by the cast      38      6   \n",
       "\n",
       "   words_not_stopword                                       tagged_words  \\\n",
       "0                   4  [(boring, NN), (plot, NN), (and, CC), (unconvi...   \n",
       "1                   4  [(the, DT), (acting, NN), (was, VBD), (superb,...   \n",
       "2                   4  [(i, NNS), (have, VBP), (mixed, VBN), (feeling...   \n",
       "3                   4  [(not, RB), (great, JJ), (not, RB), (terrible,...   \n",
       "4                   3  [(an, DT), (outstanding, JJ), (performance, NN...   \n",
       "\n",
       "   adjective_count  noun_count  verb_count  avg_word_length  commas  \n",
       "0                1           3           0         8.000000       0  \n",
       "1                0           3           2         6.250000       0  \n",
       "2                0           4           3         6.000000       0  \n",
       "3                3           1           0         6.000000       1  \n",
       "4                1           2           0         8.666667       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start of basic feature engineering \n",
    "# count the number of words in each row, the number of characters, the number of nonstop words, and the number of commas \n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "#creating a function to encapsulate preprocessing, to make it easy to replicate on  submission data\n",
    "def processing(df):\n",
    "    # making a new column in the dataframe that will contain the processed text \n",
    "    # for each entry in the \"text\" column lowercase the text and remove punctuation using regular expression \n",
    "    # lowering and removing punctuation\n",
    "    df['processed'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]','', x.lower()))\n",
    "    \n",
    "    #numerical feature engineering\n",
    "    #total length of sentence\n",
    "    df['length'] = df['processed'].apply(lambda x: len(x))\n",
    "    \n",
    "    #get number of words\n",
    "    df['words'] = df['processed'].apply(lambda x: len(x.split(' ')))\n",
    "    df['words_not_stopword'] = df['processed'].apply(lambda x: len([t for t in x.split(' ') if t not in stopWords]))\n",
    "\n",
    "    # adding new features \n",
    "    # 1. number of adjectives, nouns, verbs \n",
    "    df['tagged_words'] = df['processed'].apply(lambda x: nltk.pos_tag(nltk.word_tokenize(x)))\n",
    "    df['adjective_count'] = df['tagged_words'].apply(lambda x: sum(1 for word, tag in x if tag.startswith('JJ')))\n",
    "    df['noun_count'] = df['tagged_words'].apply(lambda x: sum(1 for word, tag in x if tag.startswith('NN')))\n",
    "    df['verb_count'] = df['tagged_words'].apply(lambda x: sum(1 for word, tag in x if tag.startswith('VB')))\n",
    "\n",
    "    #for i in range (5):\n",
    "        #print(i, \": \", tagged_words[i], \"\\n\")\n",
    "    \n",
    "    # get the average word length\n",
    "    df['avg_word_length'] = df['processed'].apply(lambda x: np.mean([len(t) for t in x.split(' ') if t not in stopWords]) if len([len(t) for t in x.split(' ') if t not in stopWords]) > 0 else 0)\n",
    "    \n",
    "    # get the average word length\n",
    "    df['commas'] = df['text'].apply(lambda x: x.count(','))\n",
    "\n",
    "    return(df)\n",
    "\n",
    "df = processing(df)\n",
    "df.shape[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "468f6d8d-a6d3-4e7b-b2e8-9644c826afde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  ['processed', 'length', 'words', 'words_not_stopword', 'tagged_words', 'adjective_count', 'noun_count', 'verb_count', 'avg_word_length', 'commas']\n",
      "\n",
      "numeric features:  ['length', 'words', 'words_not_stopword', 'tagged_words', 'adjective_count', 'noun_count', 'verb_count', 'avg_word_length', 'commas']\n",
      "\n",
      "target: \n",
      " 0     Negative\n",
      "1     Positive\n",
      "2      Neutral\n",
      "3      Neutral\n",
      "4     Positive\n",
      "5     Positive\n",
      "6      Neutral\n",
      "7     Negative\n",
      "8     Negative\n",
      "9     Positive\n",
      "10    Negative\n",
      "Name: sentiment, dtype: object\n",
      "\n",
      " dffeatures.head\n",
      "                                            processed  length  words  \\\n",
      "0            boring plot and unconvincing characters      39      5   \n",
      "1             the acting was superb a mustwatch film      38      7   \n",
      "2  i have mixed feelings about this film it had i...      56     11   \n",
      "3          not great not terrible its a decent watch      41      8   \n",
      "4             an outstanding performance by the cast      38      6   \n",
      "\n",
      "   words_not_stopword                                       tagged_words  \\\n",
      "0                   4  [(boring, NN), (plot, NN), (and, CC), (unconvi...   \n",
      "1                   4  [(the, DT), (acting, NN), (was, VBD), (superb,...   \n",
      "2                   4  [(i, NNS), (have, VBP), (mixed, VBN), (feeling...   \n",
      "3                   4  [(not, RB), (great, JJ), (not, RB), (terrible,...   \n",
      "4                   3  [(an, DT), (outstanding, JJ), (performance, NN...   \n",
      "\n",
      "   adjective_count  noun_count  verb_count  avg_word_length  commas  \n",
      "0                1           3           0         8.000000       0  \n",
      "1                0           3           2         6.250000       0  \n",
      "2                0           4           3         6.000000       0  \n",
      "3                3           1           0         6.000000       1  \n",
      "4                1           2           0         8.666667       0  \n",
      "\n",
      "dfnumerica_dfeatures.head\n",
      "    length  words  words_not_stopword  \\\n",
      "0      39      5                   4   \n",
      "1      38      7                   4   \n",
      "2      56     11                   4   \n",
      "3      41      8                   4   \n",
      "4      38      6                   3   \n",
      "\n",
      "                                        tagged_words  adjective_count  \\\n",
      "0  [(boring, NN), (plot, NN), (and, CC), (unconvi...                1   \n",
      "1  [(the, DT), (acting, NN), (was, VBD), (superb,...                0   \n",
      "2  [(i, NNS), (have, VBP), (mixed, VBN), (feeling...                0   \n",
      "3  [(not, RB), (great, JJ), (not, RB), (terrible,...                3   \n",
      "4  [(an, DT), (outstanding, JJ), (performance, NN...                1   \n",
      "\n",
      "   noun_count  verb_count  avg_word_length  commas  \n",
      "0           3           0         8.000000       0  \n",
      "1           3           2         6.250000       0  \n",
      "2           4           3         6.000000       0  \n",
      "3           1           0         6.000000       1  \n",
      "4           2           0         8.666667       0   \n",
      "\n",
      "\n",
      " X_train head\n",
      "                                            processed  length  words  \\\n",
      "2  i have mixed feelings about this film it had i...      56     11   \n",
      "1             the acting was superb a mustwatch film      38      7   \n",
      "8                  the worst movie ive seen in years      33      7   \n",
      "4             an outstanding performance by the cast      38      6   \n",
      "7       terrible acting i couldnt wait for it to end      44      9   \n",
      "\n",
      "   words_not_stopword                                       tagged_words  \\\n",
      "2                   4  [(i, NNS), (have, VBP), (mixed, VBN), (feeling...   \n",
      "1                   4  [(the, DT), (acting, NN), (was, VBD), (superb,...   \n",
      "8                   5  [(the, DT), (worst, JJS), (movie, NN), (ive, J...   \n",
      "4                   3  [(an, DT), (outstanding, JJ), (performance, NN...   \n",
      "7                   5  [(terrible, JJ), (acting, VBG), (i, NN), (coul...   \n",
      "\n",
      "   adjective_count  noun_count  verb_count  avg_word_length  commas  \n",
      "2                0           4           3         6.000000       0  \n",
      "1                0           3           2         6.250000       0  \n",
      "8                2           2           1         4.400000       0  \n",
      "4                1           2           0         8.666667       0  \n",
      "7                1           2           3         5.600000       0  \n",
      "\n",
      " X_train shape\n",
      " (7, 10)\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features= [c for c in df.columns.values if c not in ['id','text','sentiment']]\n",
    "numeric_features= [c for c in df.columns.values if c  not in ['id','text','sentiment','processed']]\n",
    "target = 'sentiment'\n",
    "\n",
    "print(\"features: \", features)\n",
    "print(\"\\nnumeric features: \", numeric_features)\n",
    "print(\"\\ntarget: \\n\", df[target])\n",
    "\n",
    "print(\"\\n dffeatures.head\\n\",df[features].head())\n",
    "print(\"\\ndfnumerica_dfeatures.head\\n\",df[numeric_features].head(), \"\\n\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], test_size=0.33, random_state=42)\n",
    "type(X_train)\n",
    "print(\"\\n X_train head\\n\",X_train.head())\n",
    "print(\"\\n X_train shape\\n\",X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f81915-11de-4806-82a0-8d05dc95b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to return one column of dataframe given a particular key value\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on text columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer to select a single column from the data frame to perform additional transformations on\n",
    "    Use on numeric columns in the data\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34806852-1f14-43c1-9837-76c66383a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a mini pipeline that comsists of two steps \n",
    "# 1. grab particular column from dataset \n",
    "# 2. perform tf-idf on that column and return results \n",
    "# what is a transformer ?? \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = Pipeline([\n",
    "                # Text selector returns the processed column in the dataframe(the column with the clean text)\n",
    "                ('selector', TextSelector(key='processed')),\n",
    "                ('tfidf', TfidfVectorizer(stop_words='english'))\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "871ade1d-7e7b-446a-abdc-6903c4d01553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have mixed feelings about this film it had its moments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    i have mixed feelings about this film it had i...\n",
       "1               the acting was superb a mustwatch film\n",
       "8                    the worst movie ive seen in years\n",
       "4               an outstanding performance by the cast\n",
       "7         terrible acting i couldnt wait for it to end\n",
       "Name: processed, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textSel = TextSelector(key='processed')\n",
    "testDF = textSel.transform(X_train)\n",
    "\n",
    "print(testDF.iloc[0])\n",
    "testDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8db281cb-72a2-418c-a8fc-26245eeb7e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 26)\n",
      "  (0, 12)\t0.5206467559864713\n",
      "  (0, 6)\t0.43218152024617124\n",
      "  (0, 5)\t0.5206467559864713\n",
      "  (0, 11)\t0.5206467559864713\n",
      "  (1, 14)\t0.544082434129559\n",
      "  (1, 20)\t0.544082434129559\n",
      "  (1, 0)\t0.4516351457444982\n",
      "  (1, 6)\t0.4516351457444982\n",
      "  (2, 25)\t0.4618042361109319\n",
      "  (2, 18)\t0.4618042361109319\n",
      "  (2, 9)\t0.4618042361109319\n",
      "  (2, 13)\t0.38333717539523177\n",
      "  (2, 24)\t0.4618042361109319\n",
      "  (3, 1)\t0.5773502691896257\n",
      "  (3, 17)\t0.5773502691896257\n",
      "  (3, 16)\t0.5773502691896257\n",
      "  (4, 4)\t0.544082434129559\n",
      "  (4, 22)\t0.544082434129559\n",
      "  (4, 21)\t0.4516351457444982\n",
      "  (4, 0)\t0.4516351457444982\n",
      "  (5, 23)\t0.5206467559864713\n",
      "  (5, 2)\t0.5206467559864713\n",
      "  (5, 7)\t0.5206467559864713\n",
      "  (5, 21)\t0.43218152024617124\n",
      "  (6, 8)\t0.4192570829702294\n",
      "  (6, 19)\t0.4192570829702294\n",
      "  (6, 10)\t0.4192570829702294\n",
      "  (6, 3)\t0.4192570829702294\n",
      "  (6, 15)\t0.4192570829702294\n",
      "  (6, 13)\t0.3480193843688467\n",
      "{'mixed': 11, 'feelings': 5, 'film': 6, 'moments': 12, 'acting': 0, 'superb': 20, 'mustwatch': 14, 'worst': 24, 'movie': 13, 'ive': 9, 'seen': 18, 'years': 25, 'outstanding': 16, 'performance': 17, 'cast': 1, 'terrible': 21, 'wait': 22, 'end': 4, 'great': 7, 'decent': 2, 'watch': 23, 'okay': 15, 'didnt': 3, 'leave': 10, 'strong': 19, 'impression': 8}\n"
     ]
    }
   ],
   "source": [
    "# print(pipeline[:-1].get_feature_names_out())\n",
    "# print(\"before:\", X_train.head())\n",
    "res = text.fit_transform(X_train) # first pass of out X_train data through pipe line\n",
    "# print(\"\\n\\nAfter:\", X_train.head())\n",
    "print(res.get_shape())\n",
    "#13,117 vectors of dimension 21,516\n",
    "#print(\"x_train: \\n\", X_train, \"\\n\\n\")\n",
    "\n",
    "# when u print out the sparse matrix res, it will return a tuple of (row, col) values that are non\n",
    "# zero that is why it is sparse, each coloumn corresponds to a different feature (words in this case)\n",
    "# u can use the vocabulary attribute from the tfidf object to get the dicitonary [word: col index in sparse matrix]\n",
    "# using this you can reform the sentence to see it \n",
    "print(res)\n",
    "vectorizor = text.named_steps['tfidf']\n",
    "print(vectorizor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8be1af2-e694-4924-85fd-584498cd0d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.63474835],\n",
       "       [-0.67961448],\n",
       "       [-1.32249305],\n",
       "       [-0.67961448],\n",
       "       [ 0.09183979],\n",
       "       [-0.29388734],\n",
       "       [ 1.24902121]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "length.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43ad16ba-1d44-449b-88c6-f9a32d3836fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "words =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "\n",
    "### start of features other than words ??\n",
    "words_not_stopword =  Pipeline([\n",
    "                ('selector', NumberSelector(key='words_not_stopword')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "avg_word_length =  Pipeline([\n",
    "                ('selector', NumberSelector(key='avg_word_length')),\n",
    "                ('standard', StandardScaler())\n",
    "            ])\n",
    "commas =  Pipeline([\n",
    "                ('selector', NumberSelector(key='commas')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "# adding piplines for NEW FEATURES we created:\n",
    "adjective_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='adjective_count')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "noun_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='noun_count')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])\n",
    "\n",
    "verb_count =  Pipeline([\n",
    "                ('selector', NumberSelector(key='verb_count')),\n",
    "                ('standard', StandardScaler()),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c522b3fb-5ab2-4852-9e41-c7b485975dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "feats = FeatureUnion([('text', text), \n",
    "                      ('length', length),\n",
    "                      ('words', words),\n",
    "                      ('words_not_stopword', words_not_stopword),\n",
    "                      ('avg_word_length', avg_word_length),\n",
    "                      ('commas', commas),\n",
    "                      ('adjective_count', adjective_count),\n",
    "                      ('verb_count', verb_count),\n",
    "                      ('noun_count', noun_count)])\n",
    "\n",
    "#print(\"before:\", X_train.head())\n",
    "feature_processing = Pipeline([('feats', feats)])\n",
    "# fit transform returns transformed samples shape (n_samples, n_transformed_features)\n",
    "catch = feature_processing.fit_transform(X_train)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37483c-da48-410e-9c9f-4ea6a4f7b715",
   "metadata": {},
   "source": [
    "#### What Pipelining is doing: There are 28 features in total and we have 5 samples, this was gotten from using shape on the matrix returned from fit_transform. The purpose of the pipeline in this usecase is to standardize all the numbers in our feature vectors. For example if you look at the length and verb_count features, their values for each sample differ in scale. length is in the 30 and 40s and verb count is less than 5, in order to standardize this as well as the rest of the features all together we pass them throufht the pipeline which scales the numbers to be closer together and not as drastic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46a43a60-52e1-4252-ba2a-f5493ab60b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           processed  length  words  \\\n",
      "2  i have mixed feelings about this film it had i...      56     11   \n",
      "1             the acting was superb a mustwatch film      38      7   \n",
      "8                  the worst movie ive seen in years      33      7   \n",
      "4             an outstanding performance by the cast      38      6   \n",
      "7       terrible acting i couldnt wait for it to end      44      9   \n",
      "3          not great not terrible its a decent watch      41      8   \n",
      "6  the movie was okay it didnt leave a strong imp...      53     10   \n",
      "\n",
      "   words_not_stopword                                       tagged_words  \\\n",
      "2                   4  [(i, NNS), (have, VBP), (mixed, VBN), (feeling...   \n",
      "1                   4  [(the, DT), (acting, NN), (was, VBD), (superb,...   \n",
      "8                   5  [(the, DT), (worst, JJS), (movie, NN), (ive, J...   \n",
      "4                   3  [(an, DT), (outstanding, JJ), (performance, NN...   \n",
      "7                   5  [(terrible, JJ), (acting, VBG), (i, NN), (coul...   \n",
      "3                   4  [(not, RB), (great, JJ), (not, RB), (terrible,...   \n",
      "6                   6  [(the, DT), (movie, NN), (was, VBD), (okay, IN...   \n",
      "\n",
      "   adjective_count  noun_count  verb_count  avg_word_length  commas  \n",
      "2                0           4           3         6.000000       0  \n",
      "1                0           3           2         6.250000       0  \n",
      "8                2           2           1         4.400000       0  \n",
      "4                1           2           0         8.666667       0  \n",
      "7                1           2           3         5.600000       0  \n",
      "3                3           1           0         6.000000       1  \n",
      "6                1           2           3         5.833333       0  \n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "(7, 34)\n",
      "  (0, 9)\t0.4618042361109319\n",
      "  (0, 13)\t0.38333717539523177\n",
      "  (0, 18)\t0.4618042361109319\n",
      "  (0, 24)\t0.4618042361109319\n",
      "  (0, 25)\t0.4618042361109319\n",
      "  (0, 26)\t-1.322493045067161\n",
      "  (0, 27)\t-0.7717436331412904\n",
      "  (0, 28)\t0.6324555320336757\n",
      "  (0, 29)\t-1.4397880639541698\n",
      "  (0, 30)\t-0.40824829046386296\n",
      "  (0, 31)\t0.8660254037844387\n",
      "  (0, 32)\t-0.5590169943749473\n",
      "  (0, 33)\t-0.3244428422615249\n"
     ]
    }
   ],
   "source": [
    "print(X_train)\n",
    "print(type(catch))\n",
    "print(catch.shape)\n",
    "print(catch[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42e79466-2c2f-46d0-9fab-55da0a3fb5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# solver = sag for real multinomoial class \n",
    "pipeline = Pipeline([\n",
    "    ('features',feats),\n",
    "    ('classifier', LogisticRegression(multi_class = 'multinomial', random_state = 42, max_iter = 2000)),\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "preds = pipeline.predict(X_test)\n",
    "np.mean(preds == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36bdc4f-120e-470b-9f59-bc94ebb689c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9a69a38-6408-4ac6-9a37-df2d32e15a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'selector': NumberSelector(key='adjective_count'), 'standard': StandardScaler()}\n",
      "(3, 34)\n",
      "['Negative' 'Neutral' 'Positive']\n"
     ]
    }
   ],
   "source": [
    "## testing if my thinking is correct \n",
    "adj = pipeline.named_steps['features'].transformer_list[6][1]\n",
    "print(adj.named_steps)\n",
    "\n",
    "# the coefficiant matrix is 3 by 21524, 3 because we have 3 different classes to predict and we have 21k columns \n",
    "# for the number of features we have for each feature vector \n",
    "print(pipeline.named_steps['classifier'].coef_.shape)\n",
    "\n",
    "model = pipeline.named_steps['classifier']\n",
    "print(model.classes_)\n",
    "#print(\"y train:\\n\" , y_train.head())\n",
    "\n",
    "#print(pipeline.named_steps['features'].transformer_list[6][1].named_steps['standard'].get_feature_names_out())\n",
    "#print(len(pipeline.named_steps['features'].transformer_list[0][1].named_steps['tfidf'].get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea03c9-663c-4cf5-96b1-106e7922af79",
   "metadata": {},
   "source": [
    "## 1b. Printing weights learned for three new features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea1994e-0c22-4fd1-99ff-ebf61cc6f049",
   "metadata": {},
   "source": [
    "#### Note: in the above output we see that the total number of features is 21,524 and the number of features(unique words) in the first transformer in the featureUnion we created is 21516 so this means that feature 'adjective_count' is feature number 21,522 (21516+6, bc it is 6 places away from the text transformer in the transformer list). We will use this information to get the weights learned for the features adjective_count, verb_count, noun_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c27c0d5b-4f9f-4604-b724-29aa8085387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Feature Weights for EAP\n",
      "adjective_count:  -0.1366969613723863\n",
      "verb_count:  -0.599221179886205\n",
      "noun_count:  -0.33187619939253094\n",
      "\n",
      "\n",
      "New Feature Weights for HPL\n",
      "adjective_count:  0.15459057609435142\n",
      "verb_count:  0.10706833808475975\n",
      "noun_count:  -0.09587487039082906\n",
      "\n",
      "\n",
      "New Feature Weights for MWS\n",
      "adjective_count:  -0.017893614721936915\n",
      "verb_count:  0.4921528418014121\n",
      "noun_count:  0.42775106978334226\n"
     ]
    }
   ],
   "source": [
    "print(\"New Feature Weights for EAP\")\n",
    "print(\"adjective_count: \", pipeline.named_steps['classifier'].coef_[0][21521])\n",
    "print(\"verb_count: \", pipeline.named_steps['classifier'].coef_[0][21522])\n",
    "print(\"noun_count: \", pipeline.named_steps['classifier'].coef_[0][21523])\n",
    "print(\"\\n\")\n",
    "print(\"New Feature Weights for HPL\")\n",
    "print(\"adjective_count: \", pipeline.named_steps['classifier'].coef_[1][21521])\n",
    "print(\"verb_count: \", pipeline.named_steps['classifier'].coef_[1][21522])\n",
    "print(\"noun_count: \", pipeline.named_steps['classifier'].coef_[1][21523])\n",
    "print(\"\\n\")\n",
    "print(\"New Feature Weights for MWS\")\n",
    "print(\"adjective_count: \", pipeline.named_steps['classifier'].coef_[2][21521])\n",
    "print(\"verb_count: \", pipeline.named_steps['classifier'].coef_[2][21522])\n",
    "print(\"noun_count: \", pipeline.named_steps['classifier'].coef_[2][21523])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff969c63-4151-4a97-ac5c-c15a7283ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2c: printing evaluation report \n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50ae57-0b68-4865-bec8-54c02566d0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of all the hyperparameters we could finetune \n",
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7136aa0-19eb-4bbe-894b-0e2a2cb700ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a3c7e-4f4f-4c5a-bf26-81f28f0c0849",
   "metadata": {},
   "source": [
    "## 1a. Different Cross Validation Settings: 2-fold, 10-fold and 20-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4fee742c-6880-4af5-a0ce-48d3b8c6a129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;background-color: white;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;text&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                                         TextSelector(key=&#x27;processed&#x27;)),\n",
       "                                                                                        (&#x27;tfidf&#x27;,\n",
       "                                                                                         TfidfVectorizer(stop_words=&#x27;english&#x27;))])),\n",
       "                                                                       (&#x27;length&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                                         NumberSelector(key=&#x27;length&#x27;)),\n",
       "                                                                                        (&#x27;standard&#x27;,\n",
       "                                                                                         StandardScaler())])),\n",
       "                                                                       (&#x27;words&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                                         NumberS...\n",
       "                                                                                         NumberSelector(key=&#x27;noun_count&#x27;)),\n",
       "                                                                                        (&#x27;standard&#x27;,\n",
       "                                                                                         StandardScaler())]))])),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(max_iter=500,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={&#x27;classifier__class_weight&#x27;: [&#x27;balanced&#x27;],\n",
       "                         &#x27;classifier__max_iter&#x27;: [500, 1000, 1500],\n",
       "                         &#x27;classifier__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                                &#x27;liblinear&#x27;],\n",
       "                         &#x27;features__text__tfidf__max_df&#x27;: [0.9, 0.95],\n",
       "                         &#x27;features__text__tfidf__ngram_range&#x27;: [(1, 1),\n",
       "                                                                (1, 2)]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-243\" type=\"checkbox\" ><label for=\"sk-estimator-id-243\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                                        FeatureUnion(transformer_list=[(&#x27;text&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                                         TextSelector(key=&#x27;processed&#x27;)),\n",
       "                                                                                        (&#x27;tfidf&#x27;,\n",
       "                                                                                         TfidfVectorizer(stop_words=&#x27;english&#x27;))])),\n",
       "                                                                       (&#x27;length&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                                         NumberSelector(key=&#x27;length&#x27;)),\n",
       "                                                                                        (&#x27;standard&#x27;,\n",
       "                                                                                         StandardScaler())])),\n",
       "                                                                       (&#x27;words&#x27;,\n",
       "                                                                        Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                                         NumberS...\n",
       "                                                                                         NumberSelector(key=&#x27;noun_count&#x27;)),\n",
       "                                                                                        (&#x27;standard&#x27;,\n",
       "                                                                                         StandardScaler())]))])),\n",
       "                                       (&#x27;classifier&#x27;,\n",
       "                                        LogisticRegression(max_iter=500,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={&#x27;classifier__class_weight&#x27;: [&#x27;balanced&#x27;],\n",
       "                         &#x27;classifier__max_iter&#x27;: [500, 1000, 1500],\n",
       "                         &#x27;classifier__solver&#x27;: [&#x27;lbfgs&#x27;, &#x27;newton-cg&#x27;,\n",
       "                                                &#x27;liblinear&#x27;],\n",
       "                         &#x27;features__text__tfidf__max_df&#x27;: [0.9, 0.95],\n",
       "                         &#x27;features__text__tfidf__ngram_range&#x27;: [(1, 1),\n",
       "                                                                (1, 2)]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-244\" type=\"checkbox\" ><label for=\"sk-estimator-id-244\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;features&#x27;,\n",
       "                 FeatureUnion(transformer_list=[(&#x27;text&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                  TextSelector(key=&#x27;processed&#x27;)),\n",
       "                                                                 (&#x27;tfidf&#x27;,\n",
       "                                                                  TfidfVectorizer(stop_words=&#x27;english&#x27;))])),\n",
       "                                                (&#x27;length&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                  NumberSelector(key=&#x27;length&#x27;)),\n",
       "                                                                 (&#x27;standard&#x27;,\n",
       "                                                                  StandardScaler())])),\n",
       "                                                (&#x27;words&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                  NumberSelector(key=&#x27;words&#x27;)),\n",
       "                                                                 (&#x27;stan...\n",
       "                                                                  NumberSelector(key=&#x27;adjective_count&#x27;)),\n",
       "                                                                 (&#x27;standard&#x27;,\n",
       "                                                                  StandardScaler())])),\n",
       "                                                (&#x27;verb_count&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                  NumberSelector(key=&#x27;verb_count&#x27;)),\n",
       "                                                                 (&#x27;standard&#x27;,\n",
       "                                                                  StandardScaler())])),\n",
       "                                                (&#x27;noun_count&#x27;,\n",
       "                                                 Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                                  NumberSelector(key=&#x27;noun_count&#x27;)),\n",
       "                                                                 (&#x27;standard&#x27;,\n",
       "                                                                  StandardScaler())]))])),\n",
       "                (&#x27;classifier&#x27;,\n",
       "                 LogisticRegression(max_iter=500, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-245\" type=\"checkbox\" ><label for=\"sk-estimator-id-245\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">features: FeatureUnion</label><div class=\"sk-toggleable__content\"><pre>FeatureUnion(transformer_list=[(&#x27;text&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                 TextSelector(key=&#x27;processed&#x27;)),\n",
       "                                                (&#x27;tfidf&#x27;,\n",
       "                                                 TfidfVectorizer(stop_words=&#x27;english&#x27;))])),\n",
       "                               (&#x27;length&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                 NumberSelector(key=&#x27;length&#x27;)),\n",
       "                                                (&#x27;standard&#x27;,\n",
       "                                                 StandardScaler())])),\n",
       "                               (&#x27;words&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                 NumberSelector(key=&#x27;words&#x27;)),\n",
       "                                                (&#x27;standard&#x27;,\n",
       "                                                 StandardScaler())])),\n",
       "                               (...\n",
       "                                                 StandardScaler())])),\n",
       "                               (&#x27;adjective_count&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                 NumberSelector(key=&#x27;adjective_count&#x27;)),\n",
       "                                                (&#x27;standard&#x27;,\n",
       "                                                 StandardScaler())])),\n",
       "                               (&#x27;verb_count&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                 NumberSelector(key=&#x27;verb_count&#x27;)),\n",
       "                                                (&#x27;standard&#x27;,\n",
       "                                                 StandardScaler())])),\n",
       "                               (&#x27;noun_count&#x27;,\n",
       "                                Pipeline(steps=[(&#x27;selector&#x27;,\n",
       "                                                 NumberSelector(key=&#x27;noun_count&#x27;)),\n",
       "                                                (&#x27;standard&#x27;,\n",
       "                                                 StandardScaler())]))])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>text</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-246\" type=\"checkbox\" ><label for=\"sk-estimator-id-246\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TextSelector</label><div class=\"sk-toggleable__content\"><pre>TextSelector(key=&#x27;processed&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-247\" type=\"checkbox\" ><label for=\"sk-estimator-id-247\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>length</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-248\" type=\"checkbox\" ><label for=\"sk-estimator-id-248\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;length&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-249\" type=\"checkbox\" ><label for=\"sk-estimator-id-249\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>words</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-250\" type=\"checkbox\" ><label for=\"sk-estimator-id-250\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;words&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-251\" type=\"checkbox\" ><label for=\"sk-estimator-id-251\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>words_not_stopword</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-252\" type=\"checkbox\" ><label for=\"sk-estimator-id-252\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;words_not_stopword&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-253\" type=\"checkbox\" ><label for=\"sk-estimator-id-253\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>avg_word_length</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-254\" type=\"checkbox\" ><label for=\"sk-estimator-id-254\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;avg_word_length&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-255\" type=\"checkbox\" ><label for=\"sk-estimator-id-255\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>commas</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-256\" type=\"checkbox\" ><label for=\"sk-estimator-id-256\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;commas&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-257\" type=\"checkbox\" ><label for=\"sk-estimator-id-257\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>adjective_count</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-258\" type=\"checkbox\" ><label for=\"sk-estimator-id-258\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;adjective_count&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-259\" type=\"checkbox\" ><label for=\"sk-estimator-id-259\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>verb_count</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-260\" type=\"checkbox\" ><label for=\"sk-estimator-id-260\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;verb_count&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-261\" type=\"checkbox\" ><label for=\"sk-estimator-id-261\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>noun_count</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-262\" type=\"checkbox\" ><label for=\"sk-estimator-id-262\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NumberSelector</label><div class=\"sk-toggleable__content\"><pre>NumberSelector(key=&#x27;noun_count&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-263\" type=\"checkbox\" ><label for=\"sk-estimator-id-263\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-264\" type=\"checkbox\" ><label for=\"sk-estimator-id-264\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=500, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=Pipeline(steps=[('features',\n",
       "                                        FeatureUnion(transformer_list=[('text',\n",
       "                                                                        Pipeline(steps=[('selector',\n",
       "                                                                                         TextSelector(key='processed')),\n",
       "                                                                                        ('tfidf',\n",
       "                                                                                         TfidfVectorizer(stop_words='english'))])),\n",
       "                                                                       ('length',\n",
       "                                                                        Pipeline(steps=[('selector',\n",
       "                                                                                         NumberSelector(key='length')),\n",
       "                                                                                        ('standard',\n",
       "                                                                                         StandardScaler())])),\n",
       "                                                                       ('words',\n",
       "                                                                        Pipeline(steps=[('selector',\n",
       "                                                                                         NumberS...\n",
       "                                                                                         NumberSelector(key='noun_count')),\n",
       "                                                                                        ('standard',\n",
       "                                                                                         StandardScaler())]))])),\n",
       "                                       ('classifier',\n",
       "                                        LogisticRegression(max_iter=500,\n",
       "                                                           random_state=42))]),\n",
       "             param_grid={'classifier__class_weight': ['balanced'],\n",
       "                         'classifier__max_iter': [500, 1000, 1500],\n",
       "                         'classifier__solver': ['lbfgs', 'newton-cg',\n",
       "                                                'liblinear'],\n",
       "                         'features__text__tfidf__max_df': [0.9, 0.95],\n",
       "                         'features__text__tfidf__ngram_range': [(1, 1),\n",
       "                                                                (1, 2)]})"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# since we are using logistic regression now classifier__max_depth is not a hyper parameter we can finetune\n",
    "# look at the list of hyperparameters above to see what we can finetune instead \n",
    "hyperparameters = { 'features__text__tfidf__max_df': [0.9, 0.95],\n",
    "                    'features__text__tfidf__ngram_range': [(1,1), (1,2)],\n",
    "                    'classifier__class_weight': ['balanced'],\n",
    "                    'classifier__solver': ['lbfgs', 'newton-cg', 'liblinear'],\n",
    "                    'classifier__max_iter':[500,1000,1500],\n",
    "                  }\n",
    "# this is where we do cross validation \n",
    "# we want to see what are the best tunings for our hyperparameters so we need to split the \n",
    "# data three ways so that we have a batch to see what the best hyperparameters are \n",
    "clf2 = GridSearchCV(pipeline, hyperparameters, cv=2)\n",
    "clf10 = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    "clf20 = GridSearchCV(pipeline, hyperparameters, cv=20)\n",
    " \n",
    "# Fit and tune model\n",
    "clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd81d6b-ec79-47a1-a3d0-ca3fbaf5cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2DF = pd.DataFrame(clf.cv_results_)\n",
    "print(clf2DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "dd48f006-42d6-4136-b30e-5d19dee1fb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__class_weight': 'balanced',\n",
       " 'classifier__max_iter': 500,\n",
       " 'classifier__solver': 'newton-cg',\n",
       " 'features__text__tfidf__max_df': 0.9,\n",
       " 'features__text__tfidf__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bfbf1f2c-1ee7-4189-8449-bb5e193b7a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refitting on entire training data using best settings\n",
    "clf2.refit\n",
    "\n",
    "preds = clf2.predict(X_test)\n",
    "probs = clf2.predict_proba(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3ce0a32d-8aba-4b25-a273-1a6c217308aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2-fold cross validation: 0.7816465490560198 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"2-fold cross validation:\", np.mean(preds == y_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6491bbc7-50bc-4d9e-871b-893064f3da54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf2: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.78      0.78      0.78      2587\n",
      "         HPL       0.77      0.80      0.78      1852\n",
      "         MWS       0.79      0.77      0.78      2023\n",
      "\n",
      "    accuracy                           0.78      6462\n",
      "   macro avg       0.78      0.78      0.78      6462\n",
      "weighted avg       0.78      0.78      0.78      6462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing Evaluation report \n",
    "print(\"clf2: \\n\", classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "08d3c045-6acb-4462-99ea-d1e1914afdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf10: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.78      0.78      0.78      2587\n",
      "         HPL       0.77      0.80      0.78      1852\n",
      "         MWS       0.79      0.77      0.78      2023\n",
      "\n",
      "    accuracy                           0.78      6462\n",
      "   macro avg       0.78      0.78      0.78      6462\n",
      "weighted avg       0.78      0.78      0.78      6462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf10.fit(X_train, y_train)\n",
    "clf10.best_params_\n",
    "clf10.refit\n",
    "\n",
    "preds = clf10.predict(X_test)\n",
    "probs = clf10.predict_proba(X_test)\n",
    "\n",
    "np.mean(preds == y_test)\n",
    "print(\"clf10: \\n\", classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d179bdbc-03db-4e34-aa95-2b872a4ac75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold cross validation: 0.7818012999071495 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"10-fold cross validation:\", np.mean(preds == y_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f51bceff-2092-4b06-920c-077e212816ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf20: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.78      0.78      0.78      2587\n",
      "         HPL       0.77      0.80      0.78      1852\n",
      "         MWS       0.79      0.77      0.78      2023\n",
      "\n",
      "    accuracy                           0.78      6462\n",
      "   macro avg       0.78      0.78      0.78      6462\n",
      "weighted avg       0.78      0.78      0.78      6462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf20.fit(X_train, y_train)\n",
    "clf20.best_params_\n",
    "clf20.refit\n",
    "\n",
    "preds = clf20.predict(X_test)\n",
    "probs = clf20.predict_proba(X_test)\n",
    "\n",
    "print(\"clf20: \\n\", classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ce27fed3-fa54-4a5e-8488-23b2ae1fbbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-fold cross validation: 0.7816465490560198 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"20-fold cross validation:\", np.mean(preds == y_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853de492-d72a-4b5d-b296-91b4adb76afd",
   "metadata": {},
   "source": [
    "## 1b. Feature Importance: printing out the 10 most important and least important features for each class, EAP, HPL, and MWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "705a3618-6f84-4ea4-bc37-d7ed4f5804d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'MWS'] \n",
      "\n",
      "Top 10 Features for Class EAP:\n",
      "1. Feature: mr, Weight: 2.2072667836715794\n",
      "2. Feature: madame, Weight: 2.1814090246927336\n",
      "3. Feature: gentleman, Weight: 2.0128172870292076\n",
      "4. Feature: balloon, Weight: 1.8273713598905108\n",
      "5. Feature: minutes, Weight: 1.8248577255162723\n",
      "6. Feature: lady, Weight: 1.8125028671224843\n",
      "7. Feature: altogether, Weight: 1.7457028808238926\n",
      "8. Feature: dupin, Weight: 1.743151411316225\n",
      "9. Feature: matter, Weight: 1.7083742939592135\n",
      "10. Feature: character, Weight: 1.6302574776783456\n",
      "\n",
      "\n",
      "Top 10 Features for Class HPL:\n",
      "1. Feature: west, Weight: 2.5471953018543907\n",
      "2. Feature: street, Weight: 2.3649235531347683\n",
      "3. Feature: later, Weight: 2.3412122546684406\n",
      "4. Feature: gilman, Weight: 2.270183461978333\n",
      "5. Feature: innsmouth, Weight: 2.0153998008812235\n",
      "6. Feature: men, Weight: 1.9948854499674662\n",
      "7. Feature: despite, Weight: 1.940749033477425\n",
      "8. Feature: ancient, Weight: 1.8741202240029613\n",
      "9. Feature: outside, Weight: 1.8584574038628214\n",
      "10. Feature: jermyn, Weight: 1.839208640403478\n",
      "\n",
      "\n",
      "Top 10 Features for Class MWS:\n",
      "1. Feature: raymond, Weight: 4.304499944391855\n",
      "2. Feature: perdita, Weight: 3.4348507918329796\n",
      "3. Feature: adrian, Weight: 3.1490430940736513\n",
      "4. Feature: love, Weight: 2.9123933563428257\n",
      "5. Feature: idris, Weight: 2.764990601372769\n",
      "6. Feature: miserable, Weight: 2.3618565957068194\n",
      "7. Feature: windsor, Weight: 2.2116992536689515\n",
      "8. Feature: elizabeth, Weight: 2.208978264770723\n",
      "9. Feature: misery, Weight: 2.207260426262498\n",
      "10. Feature: plague, Weight: 2.199925167872547\n",
      "\n",
      "\n",
      "Bottom 10 Features for Class EAP:\n",
      "1. Feature: raymond, Weight: -2.6873720028094903\n",
      "2. Feature: perdita, Weight: -2.0870771913670487\n",
      "3. Feature: fear, Weight: -1.9538467983504495\n",
      "4. Feature: adrian, Weight: -1.8924830599563793\n",
      "5. Feature: father, Weight: -1.8224895530173666\n",
      "6. Feature: reality, Weight: -1.7193347891422786\n",
      "7. Feature: men, Weight: -1.6727028317409083\n",
      "8. Feature: come, Weight: -1.6193828141023703\n",
      "9. Feature: england, Weight: -1.517508966835182\n",
      "10. Feature: idris, Weight: -1.4815162351717388\n",
      "\n",
      "\n",
      "Bottom 10 Features for Class HPL:\n",
      "1. Feature: heart, Weight: -2.478963415834889\n",
      "2. Feature: love, Weight: -2.1320093344974267\n",
      "3. Feature: sun, Weight: -1.7646928807543782\n",
      "4. Feature: length, Weight: -1.6468188962553287\n",
      "5. Feature: raymond, Weight: -1.6171279415824158\n",
      "6. Feature: thousand, Weight: -1.584898566325758\n",
      "7. Feature: let, Weight: -1.584873110038002\n",
      "8. Feature: feelings, Weight: -1.5250623918583206\n",
      "9. Feature: spirit, Weight: -1.4541844363051541\n",
      "10. Feature: alas, Weight: -1.4235435325712233\n",
      "\n",
      "\n",
      "Bottom 10 Features for Class MWS:\n",
      "1. Feature: thing, Weight: -2.0323559952703016\n",
      "2. Feature: things, Weight: -1.7277340753616102\n",
      "3. Feature: say, Weight: -1.694126677314041\n",
      "4. Feature: certain, Weight: -1.6797648251305006\n",
      "5. Feature: mr, Weight: -1.5509730339929915\n",
      "6. Feature: having, Weight: -1.511134875093343\n",
      "7. Feature: sure, Weight: -1.4838653042017453\n",
      "8. Feature: known, Weight: -1.447606877601292\n",
      "9. Feature: seen, Weight: -1.3794071919352837\n",
      "10. Feature: vague, Weight: -1.3753046059550114\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting the logistic regression model out of the pipeline,\n",
    "# we use \"model dot x\" to access attributes of the logistic regression class in sklearn, for example, one we will be \n",
    "# using is model.coef_ to get the learned wieghts\n",
    "model = pipeline.named_steps['classifier']\n",
    "\n",
    "# getting the features out of the pipeline which is a feature union using named_steps for the same reasons as above\n",
    "features = pipeline.named_steps['features']\n",
    "\n",
    "print(model.classes_, \"\\n\")\n",
    "\n",
    "# example of how to get the learned weights of the features, since we have 3 classes \n",
    "# model.coef_[0] corresponds to the weights for class EAP\n",
    "# model.coef_[1] corresponds to the weights for class HPL ect..\n",
    "EAP_coef = model.coef_[0]\n",
    "\n",
    "# feature union has an attribute  transformer_list where we use named_steps to access\n",
    "# tfidf. TDIDF comes from the text pipeline we created earlier, it was the first pipeline we created\n",
    "# From tdif we can get the names of the feautres in this case it is the words \n",
    "feat_names = features.transformer_list[0][1].named_steps['tfidf'].get_feature_names_out()\n",
    "\n",
    "# Create dictionaries to store the top features for each class\n",
    "top_features = {class_label: [] for class_label in model.classes_}\n",
    "bottom_features = {class_label: [] for class_label in model.classes_}\n",
    "\n",
    "# Loop through each class\n",
    "for class_label, class_coef in zip(model.classes_, model.coef_):\n",
    "    feature_coef_pairs = list(zip(feat_names, class_coef)) # pair together the word with its weight \n",
    "    feature_coef_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # store the top 10 features for the current class\n",
    "    top_features[class_label] = feature_coef_pairs[:10]\n",
    "\n",
    "    # store bottom ten features for the class by first reversing the order then taking the top ten \n",
    "    feature_coef_pairs.sort(key=lambda x: x[1], reverse=False)\n",
    "    bottom_features[class_label] = feature_coef_pairs[:10]\n",
    "\n",
    "# Print the top 10 features for each class\n",
    "for class_label, top_feats in top_features.items():\n",
    "    print(f\"Top 10 Features for Class {class_label}:\")\n",
    "    for i, (feature_name, coefficient) in enumerate(top_feats, 1):\n",
    "        print(f\"{i}. Feature: {feature_name}, Weight: {coefficient}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Print the bottom 10 features for each class\n",
    "for class_label, bottom_feats in bottom_features.items():\n",
    "    print(f\"Bottom 10 Features for Class {class_label}:\")\n",
    "    for i, (feature_name, coefficient) in enumerate(bottom_feats, 1):\n",
    "        print(f\"{i}. Feature: {feature_name}, Weight: {coefficient}\")\n",
    "    print(\"\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30421f8b-68e7-47ff-9d78-e54d9ecfbf62",
   "metadata": {},
   "source": [
    "### 1c. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6fb3eba-edae-44fb-83d7-a77816e76d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Incorrect Predicitons---\n",
      "1  predicted: \t HPL \t ground truth:  MWS\n",
      "feature vector:\n",
      " processed             the gigantic magnitude and the immediately ava...\n",
      "length                                                              124\n",
      "words                                                                20\n",
      "words_not_stopword                                                   11\n",
      "tagged_words          [(the, DT), (gigantic, JJ), (magnitude, NN), (...\n",
      "adjective_count                                                       2\n",
      "noun_count                                                            4\n",
      "verb_count                                                            3\n",
      "avg_word_length                                                7.181818\n",
      "commas                                                                1\n",
      "Name: id15695, dtype: object\n",
      "\n",
      "\n",
      "2  predicted: \t EAP \t ground truth:  MWS\n",
      "feature vector:\n",
      " processed             shall i disturb this calm by mingling in the w...\n",
      "length                                                               50\n",
      "words                                                                10\n",
      "words_not_stopword                                                    5\n",
      "tagged_words          [(shall, MD), (i, VB), (disturb, NN), (this, D...\n",
      "adjective_count                                                       0\n",
      "noun_count                                                            3\n",
      "verb_count                                                            2\n",
      "avg_word_length                                                     5.8\n",
      "commas                                                                0\n",
      "Name: id07954, dtype: object\n",
      "\n",
      "\n",
      "3  predicted: \t MWS \t ground truth:  EAP\n",
      "feature vector:\n",
      " processed             he had seen so many customs and witnessed so g...\n",
      "length                                                              399\n",
      "words                                                                69\n",
      "words_not_stopword                                                   33\n",
      "tagged_words          [(he, PRP), (had, VBD), (seen, VBN), (so, RB),...\n",
      "adjective_count                                                       8\n",
      "noun_count                                                           17\n",
      "verb_count                                                           11\n",
      "avg_word_length                                                6.909091\n",
      "commas                                                                1\n",
      "Name: id16303, dtype: object\n",
      "\n",
      "\n",
      "4  predicted: \t MWS \t ground truth:  EAP\n",
      "feature vector:\n",
      " processed             we went up stairs into the chamber where the b...\n",
      "length                                                              128\n",
      "words                                                                23\n",
      "words_not_stopword                                                   10\n",
      "tagged_words          [(we, PRP), (went, VBD), (up, RP), (stairs, NN...\n",
      "adjective_count                                                       1\n",
      "noun_count                                                            4\n",
      "verb_count                                                            6\n",
      "avg_word_length                                                     6.3\n",
      "commas                                                                1\n",
      "Name: id07932, dtype: object\n",
      "\n",
      "\n",
      "5  predicted: \t EAP \t ground truth:  HPL\n",
      "feature vector:\n",
      " processed             over those horrors the evil moon now hung very...\n",
      "length                                                              105\n",
      "words                                                                23\n",
      "words_not_stopword                                                   11\n",
      "tagged_words          [(over, IN), (those, DT), (horrors, NNS), (the...\n",
      "adjective_count                                                       3\n",
      "noun_count                                                            5\n",
      "verb_count                                                            3\n",
      "avg_word_length                                                4.272727\n",
      "commas                                                                1\n",
      "Name: id20875, dtype: object\n",
      "\n",
      "\n",
      "6  predicted: \t MWS \t ground truth:  EAP\n",
      "feature vector:\n",
      " processed             she listened to me as she had done to the narr...\n",
      "length                                                              279\n",
      "words                                                                56\n",
      "words_not_stopword                                                   18\n",
      "tagged_words          [(she, PRP), (listened, VBD), (to, TO), (me, P...\n",
      "adjective_count                                                       2\n",
      "noun_count                                                           10\n",
      "verb_count                                                           11\n",
      "avg_word_length                                                6.722222\n",
      "commas                                                                5\n",
      "Name: id14743, dtype: object\n",
      "\n",
      "\n",
      "7  predicted: \t EAP \t ground truth:  HPL\n",
      "feature vector:\n",
      " processed             his chief amusements were gunning and fishing ...\n",
      "length                                                              214\n",
      "words                                                                35\n",
      "words_not_stopword                                                   17\n",
      "tagged_words          [(his, PRP$), (chief, NN), (amusements, NNS), ...\n",
      "adjective_count                                                       1\n",
      "noun_count                                                           11\n",
      "verb_count                                                            6\n",
      "avg_word_length                                                7.470588\n",
      "commas                                                                2\n",
      "Name: id07281, dtype: object\n",
      "\n",
      "\n",
      "8  predicted: \t MWS \t ground truth:  HPL\n",
      "feature vector:\n",
      " processed                  i have not the slightest fear for the result\n",
      "length                                                               44\n",
      "words                                                                 9\n",
      "words_not_stopword                                                    3\n",
      "tagged_words          [(i, NNS), (have, VBP), (not, RB), (the, DT), ...\n",
      "adjective_count                                                       1\n",
      "noun_count                                                            3\n",
      "verb_count                                                            1\n",
      "avg_word_length                                                6.333333\n",
      "commas                                                                0\n",
      "Name: id09240, dtype: object\n",
      "\n",
      "\n",
      "9  predicted: \t EAP \t ground truth:  MWS\n",
      "feature vector:\n",
      " processed             i will content myself with saying in addition ...\n",
      "length                                                              166\n",
      "words                                                                30\n",
      "words_not_stopword                                                   12\n",
      "tagged_words          [(i, NN), (will, MD), (content, VB), (myself, ...\n",
      "adjective_count                                                       3\n",
      "noun_count                                                            9\n",
      "verb_count                                                            5\n",
      "avg_word_length                                                7.083333\n",
      "commas                                                                5\n",
      "Name: id23995, dtype: object\n",
      "\n",
      "\n",
      "10  predicted: \t EAP \t ground truth:  MWS\n",
      "feature vector:\n",
      " processed                                 but my efforts were fruitless\n",
      "length                                                               29\n",
      "words                                                                 5\n",
      "words_not_stopword                                                    2\n",
      "tagged_words          [(but, CC), (my, PRP$), (efforts, NNS), (were,...\n",
      "adjective_count                                                       1\n",
      "noun_count                                                            1\n",
      "verb_count                                                            1\n",
      "avg_word_length                                                     8.0\n",
      "commas                                                                0\n",
      "Name: id15141, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "incorrectly_predicted_indices = np.where(preds != y_test)[0]\n",
    "print(\"---Incorrect Predicitons---\")\n",
    "for i in range (10):\n",
    "    print(i+1 , \" predicted: \\t\", preds[incorrectly_predicted_indices[i]], \"\\t ground truth: \", y_test[incorrectly_predicted_indices[i]])\n",
    "    print(\"feature vector:\\n\",  X_test.iloc[i])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d61aa16e-dcae-441b-829a-2f21d3d4cbb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id02310</th>\n",
       "      <td>0.199965</td>\n",
       "      <td>0.063969</td>\n",
       "      <td>0.736066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id24541</th>\n",
       "      <td>0.795038</td>\n",
       "      <td>0.048417</td>\n",
       "      <td>0.156545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id00134</th>\n",
       "      <td>0.190049</td>\n",
       "      <td>0.748933</td>\n",
       "      <td>0.061018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27757</th>\n",
       "      <td>0.648430</td>\n",
       "      <td>0.233289</td>\n",
       "      <td>0.118280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id04081</th>\n",
       "      <td>0.620507</td>\n",
       "      <td>0.274016</td>\n",
       "      <td>0.105477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              EAP       HPL       MWS\n",
       "id                                   \n",
       "id02310  0.199965  0.063969  0.736066\n",
       "id24541  0.795038  0.048417  0.156545\n",
       "id00134  0.190049  0.748933  0.061018\n",
       "id27757  0.648430  0.233289  0.118280\n",
       "id04081  0.620507  0.274016  0.105477"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('./HA9-data/test.csv')\n",
    "\n",
    "#preprocessing\n",
    "submission = processing(submission)\n",
    "predictions = clf.predict_proba(submission)\n",
    "\n",
    "preds = pd.DataFrame(data=predictions, columns = clf.best_estimator_.named_steps['classifier'].classes_)\n",
    "\n",
    "#generating a submission file\n",
    "result = pd.concat([submission[['id']], preds], axis=1)\n",
    "result.set_index('id', inplace = True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ecfd1-fbe6-4a7a-897f-26d0947f7daa",
   "metadata": {},
   "source": [
    "### Model Performance Observations:\n",
    "#### 1. After the addition of the three features the models performance went up quite significantly. The proportion of correctly classified text from the random forest version with the initial features was 0.671. After adding the three new features it went up to 0.77. I believe this is because the addition of adjective, verb and noun count better potrays the styles of each author and so it helps when classifying the text.\n",
    "#### 2. The features we were initially using, length, words, commas, are not very useful when trying to distinguish bewteen each authors writing style. Theses are all features that don't show much uniqueness in how the three authors write. Whereas the amount of verbs, adjectives and nouns tell more about the style of the writer and therfore helps the model predict the pieces of text better.\n",
    "### 3. Lastly, the change from using a random forest classifier to a logistic regression classifier might have helped the precision be a little better since from what I understand about random forest classifiers is that since they are more complex they have the potential to overfit when dealing with smaller datasets. Whereas logistic regression is a simpler model and is less prone to overfitting perhaps. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
